# PoC Spark + Apache Iceberg (Bronze ‚Üí Silver)

Prova de conceito de um pipeline **Lakehouse** usando **Apache Spark** e **Apache Iceberg**, com ingest√£o simulada via Kafka, gera√ß√£o de dados fake na camada Bronze, consolida√ß√£o gen√©rica na camada Silver via `MERGE INTO` e valida√ß√£o de consist√™ncia entre camadas.

O projeto roda **localmente via Docker** e est√° preparado para futura execu√ß√£o em **OCI Data Flow**.

---

## üß± Stack

- Apache Spark 3.4.1
- Apache Iceberg 1.4.3
- Docker
- Python (PySpark)
- Arquitetura Bronze ‚Üí Silver
- MERGE ACID (Iceberg)

---

## üìÅ Estrutura do projeto

- **common/** ‚Äì Bibliotecas Python reutiliz√°veis
- **jobs/** ‚Äì Jobs Spark (consolida√ß√£o Silver e valida√ß√µes)
- **scripts/** ‚Äì Geradores de dados fake (simula√ß√£o Kafka ‚Üí Bronze)
- **libs/** ‚Äì Bin√°rios do Spark e JARs do Iceberg
- **work/** ‚Äì Dados locais e warehouse Iceberg (n√£o versionado)
- **Dockerfile** ‚Äì Ambiente Spark local
- **entrypoint.sh** ‚Äì Inicializa√ß√£o do Spark
---
## üì¶ Download das depend√™ncias (obrigat√≥rio)

Antes do build da imagem, fa√ßa o download dos bin√°rios necess√°rios.

### Apache Spark 3.4.1

```bash
mkdir -p libs
cd libs

wget https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz

wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.3/iceberg-spark-runtime-3.4_2.12-1.4.3.jar
```
---
## üê≥ Build da imagem Docker
```
docker build --network=host -t poc-spark-delta:latest .
```
---
## ‚ñ∂Ô∏è Executando o container local
```
docker run --rm -it \
  --network=host \
  -v "$PWD/work":/work \
  -v "$PWD/common":/opt/app/common \
  -v "$PWD/jobs":/opt/app/jobs \
  -v "$PWD/scripts":/opt/app/scripts \
  poc-spark-delta:latest bash
```
---
## üîé Valida√ß√£o do ambiente Spark
Dentro do container:
```
echo $SPARK_HOME                            # /opt/spark
which spark-submit                          # /opt/spark/bin/spark-submit
spark-submit --version                      # deve imprimir Spark 3.4.1
ls $SPARK_HOME/jars | grep iceberg          # iceberg-spark-runtime-3.4_2.12-1.4.3.jar
```
---
## üß™ Gera√ß√£o de dados Bronze (entidade √∫nica)
Simula m√∫ltiplos t√≥picos Kafka.
```
spark-submit scripts/gen_fake_schema_teste.py
```
---
## üß™ Gera√ß√£o de dados Bronze (m√∫ltiplas entidades)
```
spark-submit \
  scripts/gen_fake_bronze_multi_entities.py \
  --entities cliente,produto,pedido,fatura \
  --start-date 2026-01-01 \
  --end-date 2026-01-03 \
  --rows-per-day 1000 \
  --key-space 300
```
---
## üì¶ Gerando dependency archive (libs Python)
Obrigat√≥rio para execu√ß√£o distribu√≠da do Spark.
```
cd /opt/app
rm /work/archive.zip
zip -r /work/archive.zip common -x "*/__pycache__/*"
```
---
## üîÑ Consolida√ß√£o Bronze ‚Üí Silver (Iceberg)
```
spark-submit \
  --py-files /work/archive.zip \
  --driver-memory 4g \
  --executor-memory 4g \
  --conf spark.executor.memoryOverhead=1g \
  --conf spark.sql.shuffle.partitions=8 \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.local.type=hadoop \
  --conf spark.sql.catalog.local.warehouse=/work/warehouse \
  jobs/poc_silver_iceberg_generic.py \
  --bronze-uri file:///work/bronze/schema_teste/tabela_teste \
  --catalog local \
  --database silver \
  --table tabela_teste \
  --pk idt_teste \
  --dedup-col dat_kafka \
  --partition-col dt_ref \
  --partition-range 2026-01-01,2026-01-03
```
---
## ‚úÖ Valida√ß√£o Bronze vs Silver
Verifica:

  * contagem
  * cobertura de chaves
  * consist√™ncia de valores
```
spark-submit \
  --py-files /work/archive.zip \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.local.type=hadoop \
  --conf spark.sql.catalog.local.warehouse=/work/warehouse \
  jobs/validate_bronze_vs_silver.py \
  --bronze-uri file:///work/bronze/schema_teste/tabela_teste \
  --catalog local \
  --database silver \
  --table tabela_teste \
  --pk idt_teste \
  --dedup-col dat_kafka \
  --partition-col dt_ref \
  --partition-range 2026-01-01,2026-01-03
```
---
## üß† Observa√ß√µes importantes

* O job Silver √© gen√©rico (sem regra de neg√≥cio).
* O MERGE √© feito via Apache Iceberg (ACID).
* O mesmo c√≥digo pode ser executado em OCI Data Flow sem altera√ß√µes.
* Todos os dados locais ficam em work/ (n√£o versionado).